{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import wavfile as wav\n",
    "from scipy.signal import spectrogram\n",
    "from librosa.feature import melspectrogram, mfcc\n",
    "# had to uses pip to install librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining path** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Emili\\DSIM_project\\ID\\data\\\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd() + '\\\\data\\\\'\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_feature_selection(feature_extractor, train_size, val_split):\n",
    "  \n",
    "  # Note: train size must be the same as in load_data that will be used in ML.ipynb\n",
    "\n",
    "  X_train2 = []\n",
    "  y_train2 = []\n",
    "\n",
    "  X_val = []\n",
    "  y_val = []\n",
    "\n",
    "  random.seed(10) # For reproducibility\n",
    "\n",
    "  for speaker in os.listdir(path):\n",
    "    tracks = os.listdir(path + speaker)\n",
    "    random.shuffle(tracks) # We don't want the first seconds to systematically\n",
    "                           # be train and the last to be test\n",
    "    track_num = 0\n",
    "    for track in tracks:\n",
    "      track_num = track_num + 1\n",
    "      _, signal = wav.read(path + speaker + '/' + track)\n",
    "\n",
    "      if track_num <=np.floor(train_size*len(tracks)):\n",
    "        if track_num <=np.floor(train_size*len(tracks)*(1-val_split)):\n",
    "          X_train2.append(feature_extractor(signal))\n",
    "          y_train2.append(speaker)\n",
    "        else:\n",
    "          X_val.append(feature_extractor(signal))\n",
    "          y_val.append(speaker)\n",
    "          \n",
    "  eps = 0.001\n",
    "  X_train2 = np.array(X_train2)\n",
    "  X_train2_mean = X_train2.mean(axis=0)\n",
    "  X_train2_std = X_train2.std(axis=0)\n",
    "  X_train2 = (X_train2 - X_train2_mean + eps)/(X_train2_std + eps)\n",
    "  X_train2 = [row for row in X_train2]\n",
    "  X_val = [row for row in (np.array(X_val) - X_train2_mean + eps)/(X_train2_std + eps)]\n",
    "\n",
    "\n",
    "  return X_train2, X_val, y_train2, y_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Features**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temporal features**\n",
    "\n",
    "We are going to use all these features since they are all scalar (-> they don't slow the training down compared to the frequency features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(input):\n",
    "    return np.sum((input*1.0)**2, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_dev(input):\n",
    "    return np.std(input, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcr (input):\n",
    "  k=0\n",
    "  for i in range(0, len(input)-1):\n",
    "    if input[i]*input[i+1]<0:\n",
    "      k=k+1\n",
    "\n",
    "  return np.array(k, ndmin = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency features**\n",
    "\n",
    "We are going to apply a selection process for these features since they can be pretty \"heavy\" computationally-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats_spectrogram(input, rate = 8000):\n",
    "  _, _, spec = spectrogram(input, fs = rate)\n",
    "  out_spec = spec.flatten()\n",
    "  return out_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats_melspec(input, rate = 8000):\n",
    "  input = melspectrogram(y = input*1.0, sr = rate)\n",
    "  output = input.flatten()\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats_mfcc(input, rate = 8000):\n",
    "  input = mfcc(y = input*1.0, sr = rate)\n",
    "  output = input.flatten()\n",
    "  return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature selection**\n",
    "\n",
    "We are going to use the random forest classifier to select the most important features because it is generally a fast and robust method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Frequency features used: []\n",
      "\n",
      "Accuracy on train set: 0.9998114274938714\n",
      "\n",
      "Accuracy on val set: 0.26359649122807016\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_spectrogram']\n",
      "\n",
      "Accuracy on train set: 0.9998114274938714\n",
      "\n",
      "Accuracy on val set: 0.7469298245614036\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_mfcc']\n",
      "\n",
      "Accuracy on train set: 0.9998114274938714\n",
      "\n",
      "Accuracy on val set: 0.8714912280701754\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_melspec']\n",
      "\n",
      "Accuracy on train set: 0.9998114274938714\n",
      "\n",
      "Accuracy on val set: 0.8793859649122807\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_spectrogram', 'feats_mfcc']\n",
      "\n",
      "Accuracy on train set: 0.9998114274938714\n",
      "\n",
      "Accuracy on val set: 0.8114035087719298\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_spectrogram', 'feats_melspec']\n",
      "\n",
      "Accuracy on train set: 0.9998114274938714\n",
      "\n",
      "Accuracy on val set: 0.8460526315789474\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_mfcc', 'feats_melspec']\n",
      "\n",
      "Accuracy on train set: 0.9998114274938714\n",
      "\n",
      "Accuracy on val set: 0.8833333333333333\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_spectrogram', 'feats_mfcc', 'feats_melspec']\n",
      "\n",
      "Accuracy on train set: 0.9998114274938714\n",
      "\n",
      "Accuracy on val set: 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "function_set = {feats_melspec, feats_mfcc, feats_spectrogram}\n",
    "\n",
    "for combo in range(len(function_set) + 1):\n",
    "    for subset in itertools.combinations(function_set, combo):\n",
    "        \n",
    "        def combo(input):\n",
    "            return np.concatenate([standard_dev(input),energy(input), zcr(input)] +\n",
    "                                  [f(input) for f in subset])\n",
    "        \n",
    "        X_train2, X_val, y_train2, y_val = load_data_feature_selection(feature_extractor = combo, train_size = 0.8,\n",
    "                                                     val_split = 0.3)\n",
    "        \n",
    "        \n",
    "        model = RandomForestClassifier(random_state=10)\n",
    "        model.fit(X_train2, y_train2)\n",
    "\n",
    "        predictions = model.predict(X_val)\n",
    "        \n",
    "\n",
    "        print('\\n\\nFrequency features used:', [element.__name__ for element in subset])\n",
    "\n",
    "        # Accuracy (on train2 set)\n",
    "        print(f\"\\nAccuracy on train set: {model.score(X_train2, y_train2)}\")\n",
    "\n",
    "\n",
    "        # Accuracy (on val set)\n",
    "        print(f\"\\nAccuracy on val set: {model.score(X_val, y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency features are essential (without them validation accuracy is only 0.26). Best results are obtained by only using **feats_melspec** as frequency feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combo(input):\n",
    "  return np.concatenate((standard_dev(input),energy(input), zcr(input), feats_melspec(input)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features exceeds the number of instances, that can cause overfitting. Let's try **cross-validation** (still with the random forest classifier) to make sure that the model is not overfitting (doesn't look like it by using the train-test split but it could happen that the test set randomly has a similar distribution to the train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_cv(feature_extractor, k_folds):\n",
    "\n",
    "  X= [[] for i in range(k_folds)]\n",
    "  y= [[] for i in range(k_folds)]\n",
    "  \n",
    "  output = []\n",
    "\n",
    "  random.seed(10) # For reproducibility\n",
    "\n",
    "  random.shuffle(os.listdir(path))\n",
    "  for speaker in os.listdir(path):\n",
    "    tracks = os.listdir(path + speaker)\n",
    "    random.shuffle(tracks) # We don't want the first seconds to systematically\n",
    "                           # be train and the last to be test\n",
    "                           \n",
    "                           \n",
    "    split = int(len(tracks)//k_folds)\n",
    "    k_splits = [split*i for i in range(k_folds)]\n",
    "    k_splits = k_splits + [len(tracks)]\n",
    "    track_num = 0\n",
    "    for track in tracks:\n",
    "      \n",
    "      _, signal = wav.read(path + speaker + '/' + track)\n",
    "\n",
    "      for j in range(k_folds):\n",
    "        if track_num in range(k_splits[j],k_splits[j+1]):\n",
    "          X[j].append(feature_extractor(signal))\n",
    "          y[j].append(speaker)\n",
    "          \n",
    "      track_num = track_num + 1\n",
    "          \n",
    "  eps = 0.001\n",
    "  \n",
    "  for j in range(k_folds):\n",
    "    \n",
    "    # Normalizing  input\n",
    "    X_test = np.array(X[j])\n",
    "    X_train_list_of_lists = [X[h] for h in range(k_folds) if h!=j]\n",
    "    X_train = np.array([item for sublist in X_train_list_of_lists for item in sublist])\n",
    "    X_train_mean = X_train.mean(axis=0)\n",
    "    X_train_std = X_train.std(axis=0)\n",
    "    X_train = (X_train - X_train_mean + eps)/(X_train_std + eps)\n",
    "    X_train = [row for row in X_train]\n",
    "    X_test = [row for row in (np.array(X_test) - X_train_mean + eps)/(X_train_std + eps)]\n",
    "    \n",
    "    y_test = np.array(y[j])\n",
    "    y_train_list_of_lists = [y[h] for h in range(k_folds) if h!=j]\n",
    "    y_train = [item for sublist in y_train_list_of_lists for item in sublist]\n",
    "    \n",
    "    print(j, '-th split', len(X_train), len(X_test), '\\n\\n')\n",
    "    \n",
    "    output.append([X_train, X_test, y_train, y_test])\n",
    "\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th split 7607 1895 \n",
      "\n",
      "\n",
      "1 -th split 7607 1895 \n",
      "\n",
      "\n",
      "2 -th split 7607 1895 \n",
      "\n",
      "\n",
      "3 -th split 7607 1895 \n",
      "\n",
      "\n",
      "4 -th split 7580 1922 \n",
      "\n",
      "\n",
      "Loading completed\n",
      "\n",
      "Accuracy on train set: 0.9998947400550735\n",
      "\n",
      "Accuracy on test set: 0.8885702283516237\n"
     ]
    }
   ],
   "source": [
    "k_folds = 5\n",
    "\n",
    "cv_data = load_data_cv(combo, k_folds)\n",
    "\n",
    "print('Loading completed')\n",
    "\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "\n",
    "for i in range(k_folds):\n",
    "\n",
    "    model = RandomForestClassifier(random_state=10)\n",
    "    model.fit(cv_data[i][0], cv_data[i][2])\n",
    "\n",
    "    predictions = model.predict(cv_data[i][1])\n",
    "\n",
    "    train_acc = train_acc + model.score(cv_data[i][0], cv_data[i][2])\n",
    "\n",
    "\n",
    "    test_acc = test_acc + model.score(cv_data[i][1], cv_data[i][3])\n",
    "    \n",
    "\n",
    "print(f\"\\nAccuracy on train set: {train_acc/k_folds}\")\n",
    "print(f\"\\nAccuracy on test set: {test_acc/k_folds}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see an accuracy of **0.89** is achieved, meaning that 'curse of dimensionality' is not a problem here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
