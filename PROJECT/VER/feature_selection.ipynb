{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import wavfile as wav\n",
    "from scipy.signal import spectrogram\n",
    "from librosa.feature import melspectrogram, mfcc\n",
    "# had to uses pip to install librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining path** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Emili\\DSIM_project\\VER\\data\\\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd() + '\\\\data\\\\'\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1428"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(path + '\\\\auth_speaker'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_feature_selection(feature_extractor, val_split, train_size=0.8):\n",
    "  \n",
    "  # Note: train size must be 0.8 (see preprocessing.ipynb)\n",
    "  train2 = []\n",
    "  X_train2 = []\n",
    "  y_train2 = []\n",
    "\n",
    "  val = []\n",
    "  X_val = []\n",
    "  y_val = []\n",
    "  \n",
    "  unauth_split = train_size/(23-22*train_size)\n",
    "\n",
    "  random.seed(10) # For reproducibility\n",
    "\n",
    "  \n",
    "  # Authorized speaker\n",
    "  track_num = 0\n",
    "  tracks = os.listdir(path + 'auth_speaker')\n",
    "  random.shuffle(tracks)\n",
    "  for track in tracks:\n",
    "    track_num = track_num + 1\n",
    "    _, signal = wav.read(path + 'auth_speaker\\\\' + track)\n",
    "    if track_num <=np.floor(train_size*len(tracks)):\n",
    "      if track_num <=np.floor(train_size*len(tracks)*(1-val_split)):\n",
    "        train2.append((feature_extractor(signal), 'authorized'))\n",
    "      else:\n",
    "        val.append((feature_extractor(signal), 'authorized'))\n",
    "        \n",
    "  # Impostor speaker\n",
    "  for speaker in os.listdir(path + 'unauth_speakers'):\n",
    "    track_num = 0\n",
    "    tracks = os.listdir(path + 'unauth_speakers\\\\' + speaker)\n",
    "    random.shuffle(tracks)\n",
    "    for track in tracks:\n",
    "      track_num = track_num + 1\n",
    "      _, signal = wav.read(path + 'unauth_speakers\\\\' + speaker + '\\\\' + track)\n",
    "      if track_num <=np.floor(unauth_split*len(tracks)):\n",
    "        if track_num <=np.floor(unauth_split*len(tracks)*(1-val_split)):\n",
    "          train2.append((feature_extractor(signal), 'impostor'))\n",
    "        else:\n",
    "          val.append((feature_extractor(signal), 'impostor'))\n",
    "          \n",
    "  random.shuffle(train2)\n",
    "  random.shuffle(val)\n",
    "  \n",
    "  # Separate features and labels\n",
    "  X_train2 = [row[0] for row in train2]\n",
    "  y_train2 = [row[1] for row in train2]\n",
    "  X_val = [row[0] for row in val]\n",
    "  y_val = [row[1] for row in val]\n",
    "\n",
    "\n",
    "  # Normalize          \n",
    "  eps = 0.001\n",
    "  X_train2 = np.array(X_train2)\n",
    "  X_train2_mean = X_train2.mean(axis=0)\n",
    "  X_train2_std = X_train2.std(axis=0)\n",
    "  X_train2 = (X_train2 - X_train2_mean + eps)/(X_train2_std + eps)\n",
    "  X_train2 = [row for row in X_train2]\n",
    "  X_val = [row for row in (np.array(X_val) - X_train2_mean + eps)/(X_train2_std + eps)]\n",
    "\n",
    "\n",
    "  return X_train2, X_val, y_train2, y_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Features**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temporal features**\n",
    "\n",
    "We are going to use all these features since they are all scalar (-> they don't slow the training down compared to the frequency features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(input):\n",
    "    return np.sum((input*1.0)**2, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_dev(input):\n",
    "    return np.std(input, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcr (input):\n",
    "  k=0\n",
    "  for i in range(0, len(input)-1):\n",
    "    if input[i]*input[i+1]<0:\n",
    "      k=k+1\n",
    "\n",
    "  return np.array(k, ndmin = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency features**\n",
    "\n",
    "We are going to apply a selection process for these features since they can be pretty \"heavy\" computationally-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats_spectrogram(input, rate = 8000):\n",
    "  _, _, spec = spectrogram(input, fs = rate)\n",
    "  out_spec = spec.flatten()\n",
    "  return out_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats_melspec(input, rate = 8000):\n",
    "  input = melspectrogram(y = input*1.0, sr = rate)\n",
    "  output = input.flatten()\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats_mfcc(input, rate = 8000):\n",
    "  input = mfcc(y = input*1.0, sr = rate)\n",
    "  output = input.flatten()\n",
    "  return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature selection**\n",
    "\n",
    "We are going to use the random forest classifier to select the most important features because it is generally a fast and robust method. The performance criteria will be recall on the impostor class since for the verification task the most important thing is that we don't let impostors in (the same criteria will be used for the selection of the classifiers and of the hyperparameters of the classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Frequency features used: []\n",
      "\n",
      "Accuracy on train set: 1.0\n",
      "\n",
      "Recall on impostor class, val set: 0.7130434782608696\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_melspec']\n",
      "\n",
      "Accuracy on train set: 1.0\n",
      "\n",
      "Recall on impostor class, val set: 0.8\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_spectrogram']\n",
      "\n",
      "Accuracy on train set: 1.0\n",
      "\n",
      "Recall on impostor class, val set: 0.7391304347826086\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_mfcc']\n",
      "\n",
      "Accuracy on train set: 1.0\n",
      "\n",
      "Recall on impostor class, val set: 0.8434782608695652\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_melspec', 'feats_spectrogram']\n",
      "\n",
      "Accuracy on train set: 1.0\n",
      "\n",
      "Recall on impostor class, val set: 0.8289855072463768\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_melspec', 'feats_mfcc']\n",
      "\n",
      "Accuracy on train set: 1.0\n",
      "\n",
      "Recall on impostor class, val set: 0.8347826086956521\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_spectrogram', 'feats_mfcc']\n",
      "\n",
      "Accuracy on train set: 1.0\n",
      "\n",
      "Recall on impostor class, val set: 0.8173913043478261\n",
      "\n",
      "\n",
      "Frequency features used: ['feats_melspec', 'feats_spectrogram', 'feats_mfcc']\n",
      "\n",
      "Accuracy on train set: 1.0\n",
      "\n",
      "Recall on impostor class, val set: 0.8318840579710145\n"
     ]
    }
   ],
   "source": [
    "function_set = {feats_melspec, feats_mfcc, feats_spectrogram}\n",
    "\n",
    "for combo in range(len(function_set) + 1):\n",
    "    for subset in itertools.combinations(function_set, combo):\n",
    "        \n",
    "        def combo(input):\n",
    "            return np.concatenate([standard_dev(input),energy(input), zcr(input)] +\n",
    "                                  [f(input) for f in subset])\n",
    "        \n",
    "        X_train2, X_val, y_train2, y_val = load_data_feature_selection(feature_extractor = combo,\n",
    "                                                                       train_size = 0.8,\n",
    "                                                                       val_split = 0.3)\n",
    "        \n",
    "        \n",
    "        model = RandomForestClassifier(random_state=10)\n",
    "        model.fit(X_train2, y_train2)\n",
    "\n",
    "        predictions = model.predict(X_val)\n",
    "        \n",
    "\n",
    "        print('\\n\\nFrequency features used:', [element.__name__ for element in subset])\n",
    "\n",
    "        # Accuracy (on train2 set)\n",
    "        print(f\"\\nAccuracy on train set: {model.score(X_train2, y_train2)}\")\n",
    "\n",
    "\n",
    "        # Recall on impostor class (on val set)\n",
    "        val_impostor_recall = recall_score(y_val, predictions, pos_label='impostor')\n",
    "        print(f\"\\nRecall on impostor class, val set: {val_impostor_recall}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the best frequency feature seems to be **feats_mfcc**, which is also a shorter feature vector than feats_melspec, used in the identification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combo(input):\n",
    "  return np.concatenate((standard_dev(input),energy(input), zcr(input), feats_mfcc(input)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
